{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOU6yeZoNYYe3wE5V5GjJgr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RakshithSuresh2001/CUDA/blob/main/Addition_of_elements_in_two_arrays.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CUDA GPUs have many parallel processors grouped into Streaming Multiprocessors, or SMs. Each SM can run multiple concurrent thread blocks. As an example, a Tesla P100 GPU based on the Pascal GPU Architecture has 56 SMs, each capable of supporting up to 2048 active threads. To take full advantage of all these threads, I should launch the kernel with multiple thread blocks.\n",
        "\n",
        "By now you may have guessed that the first parameter of the execution configuration specifies the number of thread blocks. Together, the blocks of parallel threads make up what is known as the grid. Since I have N elements to process, and 256 threads per block, I just need to calculate the number of blocks to get at least N threads. I simply divide N by the block size (being careful to round up in case N is not a multiple of blockSize)."
      ],
      "metadata": {
        "id": "obOkaD20FMQi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "C++ program that adds the elements of two arrays with a million elements"
      ],
      "metadata": {
        "id": "wjq0T02nFgVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile add.cpp\n",
        "\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "// function to add the elements of two arrays\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  for (int i = 0; i < n; i++)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20; // 1M elements\n",
        "\n",
        "  float *x = new float[N];\n",
        "  float *y = new float[N];\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the CPU\n",
        "  add(N, x, y);\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  delete [] x;\n",
        "  delete [] y;\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVyTNkZzEWci",
        "outputId": "cfdb7df6-cceb-4a1c-858e-3f14fb1db67a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing add.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNpH54M_RbAU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f660a8b0-b468-4ab7-d907-83958933ef27"
      },
      "source": [
        "%%shell\n",
        "g++ add.cpp -o add"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "./add"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uchX9jp1EhBM",
        "outputId": "8083bf01-5d38-40fd-fab4-18d4d0621d7a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max error: 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile add.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "// Kernel function to add the elements of two arrays\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  for (int i = 0; i < n; i++)\n",
        "    y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20\n",
        " ;\n",
        "  float *x, *y;\n",
        "\n",
        "  // Allocate Unified Memory – accessible from CPU or GPU\n",
        "  cudaMallocManaged(&x, N*sizeof(float));\n",
        "  cudaMallocManaged(&y, N*sizeof(float));\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the GPU\n",
        "  add<<<1, 1>>>(N, x, y);\n",
        "\n",
        "  // Wait for GPU to finish before accessing on host\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  cudaFree(x);\n",
        "  cudaFree(y);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fALm4t2UElh2",
        "outputId": "5d7d188f-111e-400d-dc32-fdaf7dd7ea9d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing add.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvcc add.cu -o add_cuda\n",
        "./add_cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-N_iRY4EpeA",
        "outputId": "b65a90e5-593c-452d-a12d-e22d64fca493"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : Support for offline compilation for architectures prior to '<compute/sm/lto>_75' will be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "Max error: 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvprof ./add_cuda\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nr_mDnt6EvsI",
        "outputId": "2ba166b0-8cc7-4aa1-ea83-1fcd21457147"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==3571== NVPROF is profiling process 3571, command: ./add_cuda\n",
            "Max error: 0\n",
            "==3571== Profiling application: ./add_cuda\n",
            "==3571== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  108.54ms         1  108.54ms  108.54ms  108.54ms  add(int, float*, float*)\n",
            "      API calls:   73.05%  305.27ms         2  152.63ms  48.854us  305.22ms  cudaMallocManaged\n",
            "                   25.98%  108.54ms         1  108.54ms  108.54ms  108.54ms  cudaDeviceSynchronize\n",
            "                    0.60%  2.5084ms       114  22.003us      83ns  1.3669ms  cuDeviceGetAttribute\n",
            "                    0.25%  1.0384ms         1  1.0384ms  1.0384ms  1.0384ms  cudaLaunchKernel\n",
            "                    0.12%  485.88us         2  242.94us  234.81us  251.07us  cudaFree\n",
            "                    0.00%  10.909us         1  10.909us  10.909us  10.909us  cuDeviceGetName\n",
            "                    0.00%  5.3710us         2  2.6850us     266ns  5.1050us  cuDeviceGet\n",
            "                    0.00%  2.2330us         1  2.2330us  2.2330us  2.2330us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.1520us         3     384ns      90ns     901ns  cuDeviceGetCount\n",
            "                    0.00%     686ns         1     686ns     686ns     686ns  cuDeviceTotalMem\n",
            "                    0.00%     405ns         1     405ns     405ns     405ns  cuDeviceGetUuid\n",
            "                    0.00%     364ns         1     364ns     364ns     364ns  cuModuleGetLoadingMode\n",
            "\n",
            "==3571== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "      48  170.67KB  4.0000KB  0.9961MB  8.000000MB  805.4860us  Host To Device\n",
            "      24  170.67KB  4.0000KB  0.9961MB  4.000000MB  361.8480us  Device To Host\n",
            "      12         -         -         -           -  2.835309ms  Gpu page fault groups\n",
            "Total CPU Page faults: 36\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEXRa_PjE1DN",
        "outputId": "7342e5df-6059-4918-cec0-028c30bbbcb5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Feb 10 19:42:09 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8             13W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile add_block.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "// Kernel function to add the elements of two arrays\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  int index = threadIdx.x;\n",
        "  int stride = blockDim.x;\n",
        "  for (int i = index; i < n; i += stride)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20;\n",
        "  float *x, *y;\n",
        "\n",
        "  // Allocate Unified Memory – accessible from CPU or GPU\n",
        "  cudaMallocManaged(&x, N*sizeof(float));\n",
        "  cudaMallocManaged(&y, N*sizeof(float));\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the GPU\n",
        "  add<<<1, 256>>>(N, x, y);\n",
        "\n",
        "  // Wait for GPU to finish before accessing on host\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  cudaFree(x);\n",
        "  cudaFree(y);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTzk0g_LE5Lw",
        "outputId": "cb063506-a99a-49a1-8acb-46364a5eaa65"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing add_block.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvcc add_block.cu -o add_block\n",
        "nvprof ./add_block"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvM8WZwPE88a",
        "outputId": "75944888-65b1-4275-d444-f8216f4b10e4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : Support for offline compilation for architectures prior to '<compute/sm/lto>_75' will be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==3845== NVPROF is profiling process 3845, command: ./add_block\n",
            "Max error: 0\n",
            "==3845== Profiling application: ./add_block\n",
            "==3845== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  3.4803ms         1  3.4803ms  3.4803ms  3.4803ms  add(int, float*, float*)\n",
            "      API calls:   91.73%  282.59ms         2  141.29ms  53.286us  282.53ms  cudaMallocManaged\n",
            "                    6.19%  19.058ms         1  19.058ms  19.058ms  19.058ms  cudaLaunchKernel\n",
            "                    1.13%  3.4880ms         1  3.4880ms  3.4880ms  3.4880ms  cudaDeviceSynchronize\n",
            "                    0.80%  2.4565ms       114  21.548us      87ns  1.3018ms  cuDeviceGetAttribute\n",
            "                    0.15%  458.08us         2  229.04us  216.61us  241.46us  cudaFree\n",
            "                    0.00%  12.155us         1  12.155us  12.155us  12.155us  cuDeviceGetName\n",
            "                    0.00%  4.7330us         2  2.3660us     225ns  4.5080us  cuDeviceGet\n",
            "                    0.00%  2.1000us         1  2.1000us  2.1000us  2.1000us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.3770us         3     459ns     117ns  1.0600us  cuDeviceGetCount\n",
            "                    0.00%     585ns         1     585ns     585ns     585ns  cuDeviceTotalMem\n",
            "                    0.00%     284ns         1     284ns     284ns     284ns  cuModuleGetLoadingMode\n",
            "                    0.00%     181ns         1     181ns     181ns     181ns  cuDeviceGetUuid\n",
            "\n",
            "==3845== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "      48  170.67KB  4.0000KB  0.9961MB  8.000000MB  804.5600us  Host To Device\n",
            "      24  170.67KB  4.0000KB  0.9961MB  4.000000MB  361.6560us  Device To Host\n",
            "      12         -         -         -           -  2.038234ms  Gpu page fault groups\n",
            "Total CPU Page faults: 36\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile add_grid.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "// Kernel function to add the elements of two arrays\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "  for (int i = index; i < n; i += stride)\n",
        "    y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20;\n",
        "  float *x, *y;\n",
        "\n",
        "  // Allocate Unified Memory – accessible from CPU or GPU\n",
        "  cudaMallocManaged(&x, N*sizeof(float));\n",
        "  cudaMallocManaged(&y, N*sizeof(float));\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the GPU\n",
        "  int blockSize = 256;\n",
        "  int numBlocks = (N + blockSize - 1) / blockSize;\n",
        "  add<<<numBlocks, blockSize>>>(N, x, y);\n",
        "\n",
        "  // Wait for GPU to finish before accessing on host\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  cudaFree(x);\n",
        "  cudaFree(y);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0g2d6PfFBlO",
        "outputId": "ddad41e9-35d5-4ca4-e49b-0606f243e4d5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing add_grid.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvcc add_grid.cu -o add_grid\n",
        "nvprof ./add_grid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgbPNzOjFRWo",
        "outputId": "7b6f2613-2e81-4215-da99-d14285be7aec"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : Support for offline compilation for architectures prior to '<compute/sm/lto>_75' will be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==4368== NVPROF is profiling process 4368, command: ./add_grid\n",
            "Max error: 0\n",
            "==4368== Profiling application: ./add_grid\n",
            "==4368== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  2.5771ms         1  2.5771ms  2.5771ms  2.5771ms  add(int, float*, float*)\n",
            "      API calls:   92.87%  280.65ms         2  140.33ms  50.978us  280.60ms  cudaMallocManaged\n",
            "                    5.26%  15.888ms         1  15.888ms  15.888ms  15.888ms  cudaLaunchKernel\n",
            "                    0.86%  2.5933ms         1  2.5933ms  2.5933ms  2.5933ms  cudaDeviceSynchronize\n",
            "                    0.85%  2.5812ms       114  22.641us      86ns  1.4344ms  cuDeviceGetAttribute\n",
            "                    0.15%  464.91us         2  232.45us  211.38us  253.53us  cudaFree\n",
            "                    0.00%  11.462us         1  11.462us  11.462us  11.462us  cuDeviceGetName\n",
            "                    0.00%  4.8710us         2  2.4350us     176ns  4.6950us  cuDeviceGet\n",
            "                    0.00%  2.3350us         1  2.3350us  2.3350us  2.3350us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.1360us         3     378ns     112ns     805ns  cuDeviceGetCount\n",
            "                    0.00%     673ns         1     673ns     673ns     673ns  cuDeviceTotalMem\n",
            "                    0.00%     415ns         1     415ns     415ns     415ns  cuDeviceGetUuid\n",
            "                    0.00%     261ns         1     261ns     261ns     261ns  cuModuleGetLoadingMode\n",
            "\n",
            "==4368== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "      88  93.091KB  4.0000KB  988.00KB  8.000000MB  912.5610us  Host To Device\n",
            "      24  170.67KB  4.0000KB  0.9961MB  4.000000MB  361.5940us  Device To Host\n",
            "      11         -         -         -           -  2.514452ms  Gpu page fault groups\n",
            "Total CPU Page faults: 36\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}